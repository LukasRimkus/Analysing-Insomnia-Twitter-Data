{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "xFn95NLeX5hh",
      "metadata": {
        "id": "xFn95NLeX5hh"
      },
      "source": [
        "# Analysing Twitter Data on Insomnia\n",
        "\n",
        "This is my third-year Computer Science project at the University of Manchester. \n",
        "\n",
        "This notebook contains the code to fetch tweets about insomnia using the **Twitter API** (tweepy library is ued which does that in Python) by the chosen keywords. Tweets are appended to the existing file (or created a new one if one does not exist) in both csv and json formats. You can update the names of files in the constants section if preferred.\n",
        "\n",
        "The code was developed using the **Google Colab** platform.\n",
        "\n",
        "Essential things to have to run this notebook: \n",
        "1. Set the **BASE_PATH** which is the project directory to access the datasets and other files.\n",
        "2. Obtain the **Bearer Token** to use Twitter API and add it to the .env file in the same directory.\n",
        "\n",
        "Main resources used to build this code:\n",
        "1. [Tweepy docs](https://docs.tweepy.org/en/stable/) \n",
        "2. [Twitter API](https://developer.twitter.com/en/docs/twitter-api) \n",
        "\n",
        "¬© 2023 Lukas Rimkus "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Uj58yVW0YIAh",
      "metadata": {
        "id": "Uj58yVW0YIAh"
      },
      "source": [
        "# Connect to the Google Drive\n",
        "\n",
        "Firstly, connect to the Google Drive to be able to access files from there to read and store tweets. \n",
        "\n",
        "If other platform is used to run the notebook code, then comment this out. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ky8bGIIR3-l6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky8bGIIR3-l6",
        "outputId": "1da2de75-2e8d-4e3b-86c5-fd63118aae5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files \n",
        "\n",
        "colab_path = '/content/drive'\n",
        "drive.mount(colab_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "W4XspV16YQOD",
      "metadata": {
        "id": "W4XspV16YQOD"
      },
      "source": [
        "# Install and Import Required Libraries for Tweets Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "EgnmjxVD5R9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgnmjxVD5R9c",
        "outputId": "d756b0ff-25cb-498c-9a0b-bf5056c6ae7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-cdiz2nu9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tweepy/tweepy.git /tmp/pip-req-build-cdiz2nu9\n",
            "  Resolved https://github.com/tweepy/tweepy.git to commit 6fde20d61fd21b06408dedc38f542987cf91c1bc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.13.0) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.13.0) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.13.0) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Libraries\n",
        "!pip install python-dotenv\n",
        "!pip install git+https://github.com/tweepy/tweepy.git  # install most recent version"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "BSHgCcfbYfQz",
      "metadata": {
        "id": "BSHgCcfbYfQz"
      },
      "source": [
        "## Import Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "placed-emerald",
      "metadata": {
        "id": "placed-emerald"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import tweepy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Zzi4Y03HzvYe",
      "metadata": {
        "id": "Zzi4Y03HzvYe"
      },
      "source": [
        "# Define Constants and Configurations  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "koXq3ucUd1Lb",
      "metadata": {
        "id": "koXq3ucUd1Lb"
      },
      "source": [
        "**Change BASE_PATH to your own location on Google Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "stainless-price",
      "metadata": {
        "id": "stainless-price"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/Third Year Project\"\n",
        "\n",
        "# Load environment variables where API keys are stored\n",
        "env_path = f\"{BASE_PATH}/.env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "NUMBER_OF_MAX_REQUESTS = 1000  # in every 15 minutes there can be a maximum of 450 requests (but wait on rate limit is turned on)\n",
        "NUMBER_OF_TWEETS_IN_REQUEST = 100  # between 10 - 100 (default is 10)\n",
        "\n",
        "# Two or more words within \"\" mean that they should occur together in the tweet, e.g. if keyword is \"sleep pill\", then \n",
        "# a tweet containing \"pill sleep\" will not be matched\n",
        "keywords = '(insomnia OR \"sleep deprivation\" OR \"sleep problem\" OR \"sleeping problem\" OR cantsleep OR \"sleep pill\" OR \"sleeping pill\" OR \"sleep issue\" OR \"can‚Äôt sleep\" OR melatonin OR ambien OR zolpidem OR trazadone OR teamnosleep OR sleepless OR sleepdeprived)'\n",
        "\n",
        "csv_file_name = \"data.csv\"\n",
        "json_last_id_file_name = \"start_id.json\"\n",
        "json_file_name = \"data.json\"\n",
        "sample_file_name = \"sample.json\"\n",
        "\n",
        "csv_path = f\"{BASE_PATH}/{csv_file_name}\"  # consists of all collected tweets in csv format\n",
        "json_last_id_path = f\"{BASE_PATH}/{json_last_id_file_name}\"  # path to the id of the last saved tweet to know from which tweet to collect subsequent tweets\n",
        "json_file_path = f\"{BASE_PATH}/{json_file_name}\"  # consists of all collected tweets in json format\n",
        "sample_file_path = f\"{BASE_PATH}/{sample_file_name}\"  # consists of a random sample from the dataset\n",
        "\n",
        "sample_size = 200 \n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 1500)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5qhr3SajxEp6",
      "metadata": {
        "id": "5qhr3SajxEp6"
      },
      "source": [
        "# Data Collection Code "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EI5qdIaaR4_F",
      "metadata": {
        "id": "EI5qdIaaR4_F"
      },
      "source": [
        "## Data Collector Definition\n",
        "\n",
        "This is the class used to define methods and workflow to obtain the data for required keywords and from a given timestamp. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "IaNNDnbTQZli",
      "metadata": {
        "id": "IaNNDnbTQZli"
      },
      "outputs": [],
      "source": [
        "class DataCollector:\n",
        "    \"\"\"\n",
        "    This class introduces methods which are used in collecting data from Twitter using \n",
        "    its API. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, keywords: str, json_last_id_path: str, csv_path: str, json_file_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Constructor to set required objects and variables like paths for data. \n",
        "        \"\"\"\n",
        "        self.query = self.contruct_query(keywords)  # Construct a query to be made to Twitter to collect data\n",
        "        self.json_last_id_path = json_last_id_path\n",
        "        self.csv_path = csv_path\n",
        "        self.json_file_path = json_file_path\n",
        "        self.json_last_id_file_exists = os.path.exists(self.json_last_id_path)\n",
        "        self.csv_file_exists = os.path.exists(self.csv_path)\n",
        "        self.json_file_exists = os.path.exists(self.json_file_path)\n",
        "\n",
        "        # Create the tweepy Client object to make requests and set wait on rate limit to True \n",
        "        # to be able to fetch all tweets even though it may take longer\n",
        "        self.client = tweepy.Client(os.environ.get(\"BEARER_TOKEN\"), wait_on_rate_limit=True)   \n",
        "        self.most_recent_tweet_id = 0\n",
        "\n",
        "    def contruct_query(self, keywords: str) -> str:\n",
        "        \"\"\"\n",
        "        Define a query to be made to the API.\n",
        "        Fetch only English tweets and ignore retweets\n",
        "        \"\"\"\n",
        "        query = f\"{keywords} lang:en -is:retweet\"\n",
        "        return query \n",
        "    \n",
        "    def update_keywords(self, keywords: str) -> None:\n",
        "        \"\"\"\n",
        "        Set new keywords and reconstruct a query. \n",
        "        \"\"\"\n",
        "        self.query = self.contruct_query(keywords)\n",
        "\n",
        "    def collect_data_without_saving(self, limit: int, max_results: int) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Collect data with given parameters and return the dataframe containing fetched data without storing it anywhere locally.  \n",
        "        \"\"\"\n",
        "        tweets, includes = self.fetch_data_from_twitter(limit=limit, max_results=max_results)\n",
        "        tweets_df = self.construct_tweets_dataframe(tweets, includes)\n",
        "        return tweets_df\n",
        "\n",
        "    def collect_data(self, limit: int=400, max_results: int=100) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Collect data with given parameters and return the dataframe containing fetched data.\n",
        "\n",
        "        Firstly, the last tweet id is obtained to know from which tweet we should continue asking for data.\n",
        "        Then the data is fetched from the API.\n",
        "        After that, the data is cleaned and combined to make a dataframe. \n",
        "        Finally, the data is stored locally at a chosen location.   \n",
        "        \"\"\"\n",
        "        start_id = self.get_start_id()\n",
        "        tweets, includes = self.fetch_data_from_twitter(limit=limit, max_results=max_results, start_id=start_id)\n",
        "        tweets_df = self.construct_tweets_dataframe(tweets, includes)\n",
        "        self.save_data(tweets_df)\n",
        "\n",
        "        return tweets_df\n",
        "\n",
        "    def get_start_id(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the latest collected tweet id. \n",
        "        \"\"\"\n",
        "        # If a CSV does not exist either then it means we should start collecting\n",
        "        # data from the oldest tweet as possible which is one week from now due to Twitter restrictions. \n",
        "        if not self.json_last_id_file_exists:\n",
        "            return None\n",
        "        \n",
        "        with open(self.json_last_id_path, \"r\") as file:\n",
        "            data = json.load(file)\n",
        "        \n",
        "        start_id = data[\"start_id\"]\n",
        "        return start_id\n",
        "\n",
        "    def construct_paginator(self, limit: int=400, max_results: int=100, start_id: int=None) -> tweepy.Paginator:\n",
        "        \"\"\"\n",
        "        This method constructs Tweepy object Paginator which is responsible for making requests\n",
        "        to the API to fetch data according to the give parameters. \n",
        "        It is iterated through with pages to make requests as a max of 100 tweets can be fetched per one request.  \n",
        "        \"\"\"\n",
        "        # These are required to fetch data e.g. author id, text, time, publish time and location\n",
        "        tweet_fields = [\"author_id\", \"geo\", \"id\", \"created_at\", \"text\"]\n",
        "        place_fields = [\"full_name\", \"geo\", \"id\", \"country\", \"country_code\"]\n",
        "        user_fields = [\"name\", \"username\", \"id\", \"location\"]\n",
        "        expansions = [\"geo.place_id\", \"author_id\"]\n",
        "    \n",
        "        paginator = tweepy.Paginator(\n",
        "            self.client.search_recent_tweets,\n",
        "            self.query,\n",
        "            expansions=expansions,\n",
        "            place_fields=place_fields,\n",
        "            tweet_fields=tweet_fields,\n",
        "            user_fields=user_fields,\n",
        "            max_results=max_results,\n",
        "            since_id=start_id,\n",
        "            limit=limit)\n",
        "\n",
        "        return paginator\n",
        "\n",
        "    def fetch_data_from_twitter(self, limit: int=400, max_results: int=100, start_id: int=None) -> tuple:\n",
        "        \"\"\"\n",
        "        This method iterated through a Paginator object and collects fetched data which is returned. \n",
        "        Includes dataframe consists of extra information like user locations. \n",
        "        \"\"\"\n",
        "        \n",
        "        tweets = list()\n",
        "        includes = list()\n",
        "\n",
        "        try:\n",
        "            paginator = self.construct_paginator(limit=limit, max_results=max_results, start_id=start_id)\n",
        "\n",
        "            for response in paginator:\n",
        "                tweets.extend(response.data)\n",
        "                includes.extend(response.includes[\"users\"])\n",
        "                errors = response.errors\n",
        "\n",
        "                if errors:\n",
        "                    print(\"BAD... DO SOMETHING!\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR! Message: {e}\")\n",
        "        \n",
        "        return tweets, includes\n",
        "\n",
        "    def construct_tweets_dataframe(self, tweets: list, includes: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        This method constructs a dataframe of collected cleaned tweets from tweets and includes dataframes. \n",
        "        \"\"\"\n",
        "        if not tweets:\n",
        "            print(\"None tweets were found! Try again later!\")\n",
        "            return \n",
        "\n",
        "        tweets_df = pd.DataFrame(data=tweets, columns=['author_id', 'id', 'created_at', 'text'])\n",
        "        includes_df = pd.DataFrame(data=includes)\n",
        "\n",
        "        # Handle cases when location were not found in any tweet\n",
        "        if 'location' in includes_df.columns:\n",
        "            # Save only the columns of interest\n",
        "            includes_df = includes_df[['id', 'location']]\n",
        "        else:\n",
        "            includes_df = includes_df[['id']]\n",
        "            includes_df[\"location\"] = np.nan\n",
        "\n",
        "        includes_df.rename(columns={\"id\": \"author_id\"}, inplace=True)\n",
        "\n",
        "        # Includes dataframe may contain less rows than tweets dataframe, thus it is \n",
        "        # important to merge them on the author_id to correctly link the data\n",
        "        tweets_df = pd.merge(tweets_df, includes_df, on=\"author_id\")\n",
        "\n",
        "        # Remove the id of the user to comply with the ethics application\n",
        "        tweets_df.drop(['author_id'], axis=1, inplace=True)\n",
        "\n",
        "        # Sort the tweets dataframe as they start from the most recent one, as I need to start from the oldest one,\n",
        "        # this is why the first tweet is skipped, as it already exists in the file. \n",
        "        tweets_df.sort_values(by=[\"id\"], inplace = True)\n",
        "        \n",
        "        # Take the oldest tweet id after sorting the dataset \n",
        "        self.most_recent_tweet_id = int(tweets_df[\"id\"].iloc[-1])\n",
        "\n",
        "        tweets_df.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "        # Rename and reorder columns\n",
        "        tweets_df.rename(columns={\"created_at\": \"Publish Date\", \"text\": \"Tweet\", \"location\": \"Location\"}, inplace=True)\n",
        "        tweets_df['Location'] = tweets_df['Location'].fillna(\"\")\n",
        "        tweets_df.insert(1, 'Location', tweets_df.pop(\"Location\"))\n",
        "\n",
        "        # Preprocess tweets text slightly    \n",
        "        tweets_df['Tweet'] = tweets_df['Tweet'].apply(lambda tweet: self.simple_preprocessing(tweet))\n",
        "        \n",
        "        # Remove tweets which have the same Tweet text which implies that it can \n",
        "        # be spam by one or more users\n",
        "        tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "        \n",
        "        return tweets_df\n",
        "\n",
        "    def simple_preprocessing(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        This methods does slight preprocessing of tweets before storing them. \n",
        "\n",
        "        Hyperlinks can be regarded as noise, thus they are removed. However, their \n",
        "        position within a tweet can carry some semantic information which is necessary for \n",
        "        transformer models, so they are replaced by \"url\".\n",
        "        Also, mentions of other users are removed due to ethics concerns. \n",
        "        What is more, carriages '\\r' are removed as they produced some problems when data \n",
        "        was saved in a .csv files. \n",
        "        \"\"\"\n",
        "        text = re.sub(r'http[s]?://\\S+', 'url', text)\n",
        "        text = re.sub(r'@\\S+', '', text)\n",
        "        text = text.replace('\\r', ' ')\n",
        "\n",
        "        return text\n",
        "\n",
        "    def save_data(self, tweets_df: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        This method stores collected and preprocessed tweets in provided locations. \n",
        "        \"\"\"\n",
        "        # If the file does not yet exist, then it is just created.\n",
        "        # Otherwise, an existing json file is read, then concatenated with collected ones \n",
        "        # and finally saved to the same location.\n",
        "        if not self.json_file_exists:\n",
        "            tweets_df.to_json(self.json_file_path, orient=\"records\", indent=4)\n",
        "        else:\n",
        "            json_tweets_df = pd.read_json(self.json_file_path, orient=\"records\")\n",
        "            \n",
        "            merged_json_tweets_df = pd.concat([json_tweets_df, tweets_df])\n",
        "            merged_json_tweets_df.to_json(self.json_file_path, orient=\"records\", indent=4)\n",
        "\n",
        "        if not self.csv_file_exists:\n",
        "            # create a new file if it does not exist\n",
        "            tweets_df.to_csv(self.csv_path, index = False, encoding='utf-8')\n",
        "        else:\n",
        "            # Append collected tweets to a created csv file\n",
        "            tweets_df.to_csv(self.csv_path, mode='a', index=False, header=False, encoding='utf-8')\n",
        "\n",
        "        # Take the latest tweet id\n",
        "        start_id_dict = {\"start_id\": self.most_recent_tweet_id}\n",
        "\n",
        "        # Save the last Tweet ID to a separate file. \n",
        "        with open(self.json_last_id_path, \"w\") as outfile:\n",
        "            json.dump(start_id_dict, outfile)\n",
        "\n",
        "        print(f\"{len(tweets_df)} new tweets have been saved\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "OKSXZIIg8l-R",
      "metadata": {
        "id": "OKSXZIIg8l-R"
      },
      "source": [
        "Define constants and parameters for the data collector object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "50aDvLk78lsZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50aDvLk78lsZ",
        "outputId": "b840c4bb-eff9-4582-da0a-d1d437ad8e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11 new tweets have been saved\n"
          ]
        }
      ],
      "source": [
        "data_collector = DataCollector(keywords, json_last_id_path, csv_path, json_file_path)\n",
        "tweets_df = data_collector.collect_data(limit=NUMBER_OF_MAX_REQUESTS, max_results=NUMBER_OF_TWEETS_IN_REQUEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "CYOooimqj8D3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CYOooimqj8D3",
        "outputId": "3fca4f54-be9f-4b08-c298-eeacdc0292d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-da9db8bb-6163-4cd1-a87c-a3e47b7be2f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Location</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2023-03-24 10:14:36+00:00</td>\n",
              "      <td>Denver, CO</td>\n",
              "      <td>Another sleepless night üò£</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-03-24 10:14:44+00:00</td>\n",
              "      <td></td>\n",
              "      <td>and when you can‚Äôt sleep at night, you hear my stolen lullabies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-03-24 10:14:49+00:00</td>\n",
              "      <td>Washington, USA</td>\n",
              "      <td>Can‚Äôt sleep, any pork chops looking to be put in their place? \\n\\nPaypig paysub findom findomme finsub fincuck walletrinse walletdrain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-03-24 10:14:50+00:00</td>\n",
              "      <td></td>\n",
              "      <td>i can‚Äôt sleep without my baby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-03-24 10:15:11+00:00</td>\n",
              "      <td>üèùÔ∏èüá∫üá∏üáØüá≤</td>\n",
              "      <td>I‚Äôm so stressed I can‚Äôt sleep!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da9db8bb-6163-4cd1-a87c-a3e47b7be2f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da9db8bb-6163-4cd1-a87c-a3e47b7be2f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da9db8bb-6163-4cd1-a87c-a3e47b7be2f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Publish Date         Location  \\\n",
              "10 2023-03-24 10:14:36+00:00       Denver, CO   \n",
              "9  2023-03-24 10:14:44+00:00                    \n",
              "8  2023-03-24 10:14:49+00:00  Washington, USA   \n",
              "7  2023-03-24 10:14:50+00:00                    \n",
              "6  2023-03-24 10:15:11+00:00           üèùÔ∏èüá∫üá∏üáØüá≤   \n",
              "\n",
              "                                                                                                                                     Tweet  \n",
              "10                                                                                                               Another sleepless night üò£  \n",
              "9                                                                          and when you can‚Äôt sleep at night, you hear my stolen lullabies  \n",
              "8   Can‚Äôt sleep, any pork chops looking to be put in their place? \\n\\nPaypig paysub findom findomme finsub fincuck walletrinse walletdrain  \n",
              "7                                                                                                            i can‚Äôt sleep without my baby  \n",
              "6                                                                                                          I‚Äôm so stressed I can‚Äôt sleep!!  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0aD1h921vSFM",
      "metadata": {
        "id": "0aD1h921vSFM"
      },
      "source": [
        "## Dataset Manual Correction\n",
        "\n",
        "Thse functions inside help to fix the anomalies in the data, fix any issues if emerge, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "m6g9BOHIljk7",
      "metadata": {
        "id": "m6g9BOHIljk7"
      },
      "outputs": [],
      "source": [
        "def read_csv_dataset(csv_path: str) -> tuple:\n",
        "    \"\"\"\n",
        "    This method reads a collected tweets csv file. \n",
        "    \"\"\"\n",
        "    file_exists = os.path.exists(csv_path)\n",
        "    if not file_exists:\n",
        "        print(f\"There is no file at: {csv_path}\")\n",
        "        return False, None\n",
        "\n",
        "    # Read the dataset\n",
        "    parse_dates = [\"Publish Date\"]\n",
        "    tweets_df = pd.read_csv(csv_path, encoding='utf-8', parse_dates=parse_dates, on_bad_lines=\"skip\") \n",
        "\n",
        "    return True, tweets_df\n",
        "\n",
        "\n",
        "def read_json_dataset(json_path: str) -> tuple:\n",
        "    \"\"\"\n",
        "    This method reads a collected tweets json file. \n",
        "    \"\"\"\n",
        "    file_exists = os.path.exists(json_path)\n",
        "    if not file_exists:\n",
        "        print(f\"There is no file at: {json_path}\")\n",
        "        return False, None\n",
        "\n",
        "    # Read the dataset\n",
        "    tweets_df = pd.read_json(json_path, orient=\"records\")\n",
        "\n",
        "    return True, tweets_df\n",
        "\n",
        "\n",
        "def preprocess_datasets_manually(csv_path: str, json_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Clean tweets in both .csv and .json files as given in parameters. \n",
        "    \"\"\"\n",
        "    preprocess_csv_dataset_manually(csv_path)\n",
        "    preprocess_json_dataset_manually(json_path)\n",
        "\n",
        "\n",
        "def preprocess_csv_dataset_manually(csv_path: str) -> None:\n",
        "    \"\"\"\n",
        "    This method does some preprocessing of the tweets .csv file form the given path. \n",
        "    Tweets are stored in the same location at the end.\n",
        "    \"\"\"\n",
        "    success, tweets_df = read_csv_dataset(csv_path)\n",
        "    if not success:\n",
        "        return \n",
        "\n",
        "    number_of_original_tweets = len(tweets_df)\n",
        "    print(f\"Pre-processing: Current Tweet number in the CSV dataset: {number_of_original_tweets}\")\n",
        "\n",
        "    # Remove duplicate tweets\n",
        "    tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "    \n",
        "    # Drop tweets with non-existing values\n",
        "    tweets_df.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "    # Save the results\n",
        "    tweets_df.to_csv(csv_path, index = False, encoding='utf-8')\n",
        "    number_of_preprocessed_tweets = len(tweets_df)\n",
        "    print(f\"Pre-processing: Tweets number after some CSV pre-processing: {number_of_preprocessed_tweets}\")\n",
        "\n",
        "\n",
        "def preprocess_json_dataset_manually(json_path: str) -> None:\n",
        "    \"\"\"\n",
        "    This method does some preprocessing of the tweets .json file form the given path. \n",
        "    Tweets are stored in the same location at the end.\n",
        "    \"\"\"\n",
        "    success, tweets_df = read_json_dataset(json_path)\n",
        "    if not success:\n",
        "        return \n",
        "\n",
        "    number_of_original_tweets = len(tweets_df)\n",
        "    print(f\"Pre-processing: Current Tweet number in the Json dataset: {number_of_original_tweets}\")\n",
        "\n",
        "    # Remove duplicate tweets\n",
        "    tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "    \n",
        "    # Drop tweets with non-existing values\n",
        "    tweets_df.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "    # Save the results\n",
        "    tweets_df.to_json(json_path, orient=\"records\", indent=4)\n",
        "    number_of_preprocessed_tweets = len(tweets_df)\n",
        "    print(f\"Pre-processing: Tweets number after some Json pre-processing: {number_of_preprocessed_tweets}\")\n",
        "\n",
        "\n",
        "def get_duplicate_tweets(tweets_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return duplicate (the same text) tweets in the given dataframe.\n",
        "    \"\"\"\n",
        "    return tweets_df[tweets_df.duplicated(['Tweet'], keep=\"first\")]\n",
        "\n",
        "\n",
        "def convert_to_json(csv_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Convert a .csv file to a .json one.\n",
        "    This was needed as firstly I was storing the data in .csv format, but at the end\n",
        "    I decided to switch to .json. \n",
        "    \"\"\"\n",
        "\n",
        "    success, tweets_df = read_csv_dataset(csv_path)\n",
        "    if not success:\n",
        "        print(\"ERROR!\")\n",
        "        return \n",
        "\n",
        "    json_file_path = f\"{BASE_PATH}/data.json\"\n",
        "    tweets_df.to_json(json_file_path, orient=\"records\", indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "G8G2qP5XRnCN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8G2qP5XRnCN",
        "outputId": "4d1c32f0-c38e-4620-9187-9bc01410dbe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-processing: Current Tweet number in the CSV dataset: 927469\n",
            "Pre-processing: Tweets number after some CSV pre-processing: 927457\n",
            "Pre-processing: Current Tweet number in the Json dataset: 927480\n",
            "Pre-processing: Tweets number after some Json pre-processing: 927468\n"
          ]
        }
      ],
      "source": [
        "preprocess_datasets_manually(csv_path, json_file_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "gXF_vOAiadae",
      "metadata": {
        "id": "gXF_vOAiadae"
      },
      "source": [
        "# Experimenting and Analysing Keywords\n",
        "\n",
        "Removed each keyword which also had hashtag # at the start or contained other keywords as its subset like ambien and ambien-cr. \n",
        "Also, removed these keywords (mostly because they don't introduce many tweets and they usually contain much ads, other spam): \"sleeping pill\", lunesta, intermezzo, eszopiclone.\n",
        "I found that some tweets about insomnia contained the keyword \"sleepdeprived\", thus I added this to the keywords list after revisions. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "28X5NC4oMVx4",
      "metadata": {
        "id": "28X5NC4oMVx4"
      },
      "outputs": [],
      "source": [
        "initial_keywords = '(insomnia OR \"sleep deprivation\" OR \"sleep problem\" OR #insomnia OR #cantsleep OR \"sleep pill\" OR \"sleep issue\" OR \"can‚Äôt sleep\" OR melatonin OR ambien OR ambien-cr OR zolpidem OR lunesta OR intermezzo OR trazadone OR eszopiclone OR #teamnosleep OR sleepless)'\n",
        "\n",
        "revised_keywords = '(insomnia OR \"sleep deprivation\" OR \"sleep problem\" OR \"sleeping problem\" OR cantsleep OR \"sleep pill\" OR \"sleeping pill\" OR \"sleep issue\" OR \"can‚Äôt sleep\" OR melatonin OR ambien OR zolpidem OR trazadone OR teamnosleep OR sleepless OR sleepdeprived)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "HOYKotdqAKlM",
      "metadata": {
        "id": "HOYKotdqAKlM"
      },
      "outputs": [],
      "source": [
        "def test_specific_keywords(keywords: str, limit: int=2, max_results: int=100):\n",
        "    \"\"\"\n",
        "    This method was used to analyse tweets which contained some chosen keyword. \n",
        "    I assessed with this if it is worth adding some keyword to the final list of keywords. \n",
        "    \"\"\"\n",
        "\n",
        "    # Search for all tweets with have the keyword in last week's tweets\n",
        "    data_collector.update_keywords(keywords)\n",
        "    tweets_df = data_collector.collect_data_without_saving(limit=limit, max_results=max_results)\n",
        "    number_of_tweets = len(tweets_df)\n",
        "    print(f\"Number of tweets: {number_of_tweets}\")\n",
        "\n",
        "    return tweets_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0FD-Dv3CRVkS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "0FD-Dv3CRVkS",
        "outputId": "ce64a609-de00-425b-b539-7951691efef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets: 76\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-564fb052-0337-4648-8532-95b8fbcd66fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Location</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>2023-03-17 10:53:31+00:00</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>We have two moods: Sleep is for the weak. We need to sleep for a week.\\n#WorldSleepDay #SleepGoals #SleepDeprived #SleepAwareness #Snooze #SleepMatters #SaffronTech url</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2023-03-17 12:19:47+00:00</td>\n",
              "      <td>South Holland, The Netherlands</td>\n",
              "      <td>New comic: ‚Äôùìõùìòùìöùìî ùìê ùìõùìûùìñ‚Äô üå≥üí§\\n\\nIf you like this, please hit ‚ÄòLike‚Äô üëâüíô üôè\\n\\n#webcomic #custardfist #webcomics #comic #comics #comicstrip #funny #sleep #sleeping #sleepwell #sleepdeprived #SleepDeprivation #sleepdeprivation #beautysleep #ripvanwinkle #goodsleep url</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>2023-03-17 16:01:35+00:00</td>\n",
              "      <td>Texas, US</td>\n",
              "      <td>Alphacare offers round-the-clock access to their doctors for your health needs. With 24/7 availability, you can consult with Alphacare physicians whenever you need.\\n\\n#sleepdeprivation #sleepdeprived #sleeptips #sleepbetter #healthysleep #digitalhealth #telemedicine #virtualcare url</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>2023-03-17 17:41:29+00:00</td>\n",
              "      <td></td>\n",
              "      <td>This sucks ass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>2023-03-17 17:41:59+00:00</td>\n",
              "      <td></td>\n",
              "      <td>I like this</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-564fb052-0337-4648-8532-95b8fbcd66fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-564fb052-0337-4648-8532-95b8fbcd66fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-564fb052-0337-4648-8532-95b8fbcd66fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Publish Date                        Location  \\\n",
              "76 2023-03-17 10:53:31+00:00                       New Delhi   \n",
              "75 2023-03-17 12:19:47+00:00  South Holland, The Netherlands   \n",
              "74 2023-03-17 16:01:35+00:00                       Texas, US   \n",
              "73 2023-03-17 17:41:29+00:00                                   \n",
              "72 2023-03-17 17:41:59+00:00                                   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                           Tweet  \n",
              "76                                                                                                                      We have two moods: Sleep is for the weak. We need to sleep for a week.\\n#WorldSleepDay #SleepGoals #SleepDeprived #SleepAwareness #Snooze #SleepMatters #SaffronTech url  \n",
              "75                        New comic: ‚Äôùìõùìòùìöùìî ùìê ùìõùìûùìñ‚Äô üå≥üí§\\n\\nIf you like this, please hit ‚ÄòLike‚Äô üëâüíô üôè\\n\\n#webcomic #custardfist #webcomics #comic #comics #comicstrip #funny #sleep #sleeping #sleepwell #sleepdeprived #SleepDeprivation #sleepdeprivation #beautysleep #ripvanwinkle #goodsleep url  \n",
              "74  Alphacare offers round-the-clock access to their doctors for your health needs. With 24/7 availability, you can consult with Alphacare physicians whenever you need.\\n\\n#sleepdeprivation #sleepdeprived #sleeptips #sleepbetter #healthysleep #digitalhealth #telemedicine #virtualcare url  \n",
              "73                                                                                                                                                                                                                                                                                This sucks ass  \n",
              "72                                                                                                                                                                                                                                                                                   I like this  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df = test_specific_keywords(keywords = '(\"sleepdeprived\")')\n",
        "tweets_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "kIPtZIOzitZo",
      "metadata": {
        "id": "kIPtZIOzitZo"
      },
      "source": [
        "## Generate a Random Sample of Tweets\n",
        "\n",
        "Those tweets will be annotated for Sentiment Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "rSvx5Q5L5Pxa",
      "metadata": {
        "id": "rSvx5Q5L5Pxa"
      },
      "outputs": [],
      "source": [
        "def generate_sample(json_path: str, sample_size: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This method is used to generate a random sample of tweets dataframe which are used\n",
        "    for sentiment annotation.\n",
        "    The resulting sample is saved in the given location.\n",
        "    \"\"\"\n",
        "    success, tweets_df = read_json_dataset(json_path)\n",
        "    if not success:\n",
        "        return \n",
        "    \n",
        "    sample_df = tweets_df.sample(n=sample_size)\n",
        "\n",
        "    # Add a column with the default value for the sentiment which will\n",
        "    # be changed during the annotation process\n",
        "    sample_df[\"Sentiment\"] = 0\n",
        "\n",
        "    sample_df.to_json(sample_file_path, orient=\"records\", indent=4)\n",
        "    return sample_df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "zi3uStWgA6kv",
      "metadata": {
        "id": "zi3uStWgA6kv"
      },
      "source": [
        "Uncomment the code below to create a random sample of given size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "uoZ0-Bsfroo-",
      "metadata": {
        "id": "uoZ0-Bsfroo-"
      },
      "outputs": [],
      "source": [
        "# sample_df = generate_sample(json_path=json_file_path, sample_size=sample_size)\n",
        "# sample_df.head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
